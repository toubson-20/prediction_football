"""
ğŸ”„ CONTINUOUS LEARNING PIPELINE - PHASE 4
SystÃ¨me d'apprentissage continu auto-adaptatif pour l'amÃ©lioration permanente des modÃ¨les ML
"""

import asyncio
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from abc import ABC, abstractmethod
import numpy as np
import pandas as pd
from concurrent.futures import ThreadPoolExecutor
import pickle
import os
from pathlib import Path

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class PerformanceMetrics:
    """MÃ©triques de performance pour un modÃ¨le"""
    model_id: str
    prediction_type: str
    league: str
    accuracy: float
    precision: float
    recall: float
    f1_score: float
    roc_auc: float
    log_loss: float
    roi: float  # Return on Investment
    profit_loss: float
    confidence_calibration: float
    sample_size: int
    period_start: datetime
    period_end: datetime
    
    def to_dict(self) -> Dict:
        """Conversion en dictionnaire"""
        data = asdict(self)
        data['period_start'] = self.period_start.isoformat()
        data['period_end'] = self.period_end.isoformat()
        return data

@dataclass
class RetrainingTrigger:
    """Configuration des dÃ©clencheurs de rÃ©entraÃ®nement"""
    performance_threshold: float = 0.05  # DÃ©gradation de 5%
    sample_size_threshold: int = 20  # Minimum 20 matchs
    time_threshold_days: int = 7  # Maximum 7 jours
    confidence_drift_threshold: float = 0.1  # DÃ©rive de confiance 10%
    force_retrain_days: int = 30  # RÃ©entraÃ®nement forcÃ© tous les 30 jours

class PerformanceTracker:
    """Suivi des performances des modÃ¨les"""
    
    def __init__(self, storage_path: str = "data/performance_tracking"):
        self.storage_path = Path(storage_path)
        self.storage_path.mkdir(parents=True, exist_ok=True)
        self.metrics_history: Dict[str, List[PerformanceMetrics]] = {}
        self.load_history()
    
    def load_history(self):
        """Charger l'historique des performances"""
        try:
            history_file = self.storage_path / "metrics_history.json"
            if history_file.exists():
                with open(history_file, 'r') as f:
                    data = json.load(f)
                    for model_id, metrics_list in data.items():
                        self.metrics_history[model_id] = [
                            PerformanceMetrics(
                                model_id=m['model_id'],
                                prediction_type=m['prediction_type'],
                                league=m['league'],
                                accuracy=m['accuracy'],
                                precision=m['precision'],
                                recall=m['recall'],
                                f1_score=m['f1_score'],
                                roc_auc=m['roc_auc'],
                                log_loss=m['log_loss'],
                                roi=m['roi'],
                                profit_loss=m['profit_loss'],
                                confidence_calibration=m['confidence_calibration'],
                                sample_size=m['sample_size'],
                                period_start=datetime.fromisoformat(m['period_start']),
                                period_end=datetime.fromisoformat(m['period_end'])
                            ) for m in metrics_list
                        ]
        except Exception as e:
            logger.warning(f"Impossible de charger l'historique: {e}")
    
    def save_history(self):
        """Sauvegarder l'historique"""
        try:
            history_file = self.storage_path / "metrics_history.json"
            data = {
                model_id: [m.to_dict() for m in metrics_list]
                for model_id, metrics_list in self.metrics_history.items()
            }
            with open(history_file, 'w') as f:
                json.dump(data, f, indent=2)
        except Exception as e:
            logger.error(f"Erreur sauvegarde historique: {e}")
    
    def add_performance_metrics(self, metrics: PerformanceMetrics):
        """Ajouter de nouvelles mÃ©triques"""
        if metrics.model_id not in self.metrics_history:
            self.metrics_history[metrics.model_id] = []
        
        self.metrics_history[metrics.model_id].append(metrics)
        
        # Garder seulement les 100 derniÃ¨res mÃ©triques par modÃ¨le
        self.metrics_history[metrics.model_id] = self.metrics_history[metrics.model_id][-100:]
        
        self.save_history()
        logger.info(f"MÃ©triques ajoutÃ©es pour {metrics.model_id}")
    
    def get_recent_performance(self, model_id: str, days: int = 7) -> List[PerformanceMetrics]:
        """Obtenir les performances rÃ©centes"""
        if model_id not in self.metrics_history:
            return []
        
        cutoff_date = datetime.now() - timedelta(days=days)
        return [
            m for m in self.metrics_history[model_id]
            if m.period_end >= cutoff_date
        ]
    
    def calculate_performance_drift(self, model_id: str, baseline_days: int = 30, 
                                  recent_days: int = 7) -> Dict[str, float]:
        """Calculer la dÃ©rive de performance"""
        baseline_metrics = self.get_recent_performance(model_id, baseline_days)
        recent_metrics = self.get_recent_performance(model_id, recent_days)
        
        if len(baseline_metrics) < 5 or len(recent_metrics) < 3:
            return {'drift': 0.0, 'confidence': 0.0}
        
        # Calcul de la dÃ©rive moyenne
        baseline_accuracy = np.mean([m.accuracy for m in baseline_metrics])
        recent_accuracy = np.mean([m.accuracy for m in recent_metrics])
        
        baseline_roi = np.mean([m.roi for m in baseline_metrics])
        recent_roi = np.mean([m.roi for m in recent_metrics])
        
        accuracy_drift = (baseline_accuracy - recent_accuracy) / baseline_accuracy
        roi_drift = (baseline_roi - recent_roi) / abs(baseline_roi) if baseline_roi != 0 else 0
        
        overall_drift = (accuracy_drift + roi_drift) / 2
        
        return {
            'accuracy_drift': accuracy_drift,
            'roi_drift': roi_drift, 
            'overall_drift': overall_drift,
            'baseline_accuracy': baseline_accuracy,
            'recent_accuracy': recent_accuracy,
            'baseline_roi': baseline_roi,
            'recent_roi': recent_roi,
            'confidence': min(len(recent_metrics) / 10, 1.0)
        }

class RetrainingDecisionEngine:
    """Moteur de dÃ©cision pour le rÃ©entraÃ®nement"""
    
    def __init__(self, config: RetrainingTrigger = RetrainingTrigger()):
        self.config = config
        self.last_retrain_dates: Dict[str, datetime] = {}
    
    def should_retrain(self, model_id: str, performance_tracker: PerformanceTracker) -> Dict[str, Any]:
        """DÃ©cider si un modÃ¨le doit Ãªtre rÃ©entraÃ®nÃ©"""
        decision_factors = {
            'should_retrain': False,
            'reasons': [],
            'urgency': 'low',  # low, medium, high, critical
            'estimated_improvement': 0.0
        }
        
        # VÃ©rifier la dÃ©rive de performance
        drift_analysis = performance_tracker.calculate_performance_drift(model_id)
        
        if drift_analysis['overall_drift'] > self.config.performance_threshold:
            decision_factors['should_retrain'] = True
            decision_factors['reasons'].append(
                f"DÃ©gradation performance: {drift_analysis['overall_drift']:.2%}"
            )
            decision_factors['urgency'] = 'high'
        
        # VÃ©rifier la taille de l'Ã©chantillon rÃ©cent
        recent_metrics = performance_tracker.get_recent_performance(model_id, 7)
        total_samples = sum(m.sample_size for m in recent_metrics)
        
        if total_samples >= self.config.sample_size_threshold:
            if not decision_factors['should_retrain']:
                decision_factors['should_retrain'] = True
                decision_factors['reasons'].append(f"Ã‰chantillon suffisant: {total_samples} matchs")
        
        # RÃ©entraÃ®nement forcÃ© pÃ©riodique
        last_retrain = self.last_retrain_dates.get(model_id, 
                                                  datetime.now() - timedelta(days=365))
        days_since_retrain = (datetime.now() - last_retrain).days
        
        if days_since_retrain >= self.config.force_retrain_days:
            decision_factors['should_retrain'] = True
            decision_factors['reasons'].append(
                f"RÃ©entraÃ®nement pÃ©riodique: {days_since_retrain} jours"
            )
            if days_since_retrain > self.config.force_retrain_days * 2:
                decision_factors['urgency'] = 'critical'
        
        # Estimer l'amÃ©lioration potentielle
        if decision_factors['should_retrain']:
            decision_factors['estimated_improvement'] = min(
                abs(drift_analysis.get('overall_drift', 0)) * 1.5, 0.3
            )
        
        return decision_factors

class ModelRetrainer:
    """Gestionnaire du rÃ©entraÃ®nement des modÃ¨les"""
    
    def __init__(self, models_path: str = "data/models"):
        self.models_path = Path(models_path)
        self.retrain_queue: List[Dict] = []
        self.active_retrains: Dict[str, bool] = {}
    
    async def retrain_model(self, model_id: str, retrain_type: str = "incremental") -> Dict[str, Any]:
        """RÃ©entraÃ®ner un modÃ¨le"""
        if model_id in self.active_retrains:
            return {'status': 'error', 'message': 'RÃ©entraÃ®nement dÃ©jÃ  en cours'}
        
        self.active_retrains[model_id] = True
        retrain_start = datetime.now()
        
        try:
            logger.info(f"DÃ©but rÃ©entraÃ®nement {retrain_type} pour {model_id}")
            
            # Simuler le rÃ©entraÃ®nement (remplacer par la vraie logique)
            if retrain_type == "incremental":
                await self._incremental_retrain(model_id)
            else:
                await self._full_retrain(model_id)
            
            retrain_duration = datetime.now() - retrain_start
            
            result = {
                'status': 'success',
                'model_id': model_id,
                'retrain_type': retrain_type,
                'duration_seconds': retrain_duration.total_seconds(),
                'timestamp': datetime.now().isoformat(),
                'improvement_estimated': np.random.uniform(0.02, 0.15)  # Simulation
            }
            
            logger.info(f"RÃ©entraÃ®nement terminÃ© pour {model_id}")
            return result
            
        except Exception as e:
            logger.error(f"Erreur rÃ©entraÃ®nement {model_id}: {e}")
            return {
                'status': 'error',
                'model_id': model_id,
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
        finally:
            self.active_retrains.pop(model_id, None)
    
    async def _incremental_retrain(self, model_id: str):
        """RÃ©entraÃ®nement incrÃ©mental"""
        # Simuler le processus
        await asyncio.sleep(np.random.uniform(10, 30))
        logger.info(f"RÃ©entraÃ®nement incrÃ©mental simulÃ© pour {model_id}")
    
    async def _full_retrain(self, model_id: str):
        """RÃ©entraÃ®nement complet"""
        # Simuler le processus plus long
        await asyncio.sleep(np.random.uniform(60, 180))
        logger.info(f"RÃ©entraÃ®nement complet simulÃ© pour {model_id}")

class ABTestManager:
    """Gestionnaire des tests A/B pour nouvelles fonctionnalitÃ©s"""
    
    def __init__(self):
        self.active_tests: Dict[str, Dict] = {}
        self.test_history: List[Dict] = []
    
    def create_ab_test(self, test_name: str, model_a_id: str, model_b_id: str, 
                      traffic_split: float = 0.5, duration_days: int = 7) -> str:
        """CrÃ©er un nouveau test A/B"""
        test_id = f"ab_{test_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        self.active_tests[test_id] = {
            'test_name': test_name,
            'model_a': model_a_id,
            'model_b': model_b_id,
            'traffic_split': traffic_split,
            'start_date': datetime.now(),
            'end_date': datetime.now() + timedelta(days=duration_days),
            'results_a': [],
            'results_b': []
        }
        
        logger.info(f"Test A/B crÃ©Ã©: {test_id}")
        return test_id
    
    def assign_model(self, test_id: str, user_context: Dict) -> str:
        """Assigner un modÃ¨le pour un utilisateur"""
        if test_id not in self.active_tests:
            return self.active_tests[test_id]['model_a']
        
        test = self.active_tests[test_id]
        
        # Hash de l'utilisateur pour assignment consistante
        user_hash = hash(str(user_context.get('user_id', 'anonymous'))) % 100
        
        if user_hash < test['traffic_split'] * 100:
            return test['model_a']
        else:
            return test['model_b']
    
    def record_result(self, test_id: str, model_id: str, prediction_result: Dict):
        """Enregistrer un rÃ©sultat de prÃ©diction"""
        if test_id not in self.active_tests:
            return
        
        test = self.active_tests[test_id]
        result_data = {
            'timestamp': datetime.now(),
            'prediction_result': prediction_result
        }
        
        if model_id == test['model_a']:
            test['results_a'].append(result_data)
        elif model_id == test['model_b']:
            test['results_b'].append(result_data)
    
    def analyze_test_results(self, test_id: str) -> Dict[str, Any]:
        """Analyser les rÃ©sultats d'un test A/B"""
        if test_id not in self.active_tests:
            return {'error': 'Test non trouvÃ©'}
        
        test = self.active_tests[test_id]
        results_a = test['results_a']
        results_b = test['results_b']
        
        if len(results_a) < 10 or len(results_b) < 10:
            return {
                'status': 'insufficient_data',
                'samples_a': len(results_a),
                'samples_b': len(results_b)
            }
        
        # Calculs statistiques simulÃ©s
        accuracy_a = np.random.uniform(0.6, 0.8)
        accuracy_b = np.random.uniform(0.6, 0.8)
        roi_a = np.random.uniform(0.05, 0.25)
        roi_b = np.random.uniform(0.05, 0.25)
        
        statistical_significance = abs(accuracy_a - accuracy_b) > 0.02
        
        return {
            'test_id': test_id,
            'model_a': test['model_a'],
            'model_b': test['model_b'],
            'samples_a': len(results_a),
            'samples_b': len(results_b),
            'accuracy_a': accuracy_a,
            'accuracy_b': accuracy_b,
            'roi_a': roi_a,
            'roi_b': roi_b,
            'winner': 'model_a' if accuracy_a > accuracy_b else 'model_b',
            'improvement': abs(accuracy_a - accuracy_b),
            'statistical_significance': statistical_significance,
            'recommendation': 'deploy' if statistical_significance else 'continue_test'
        }

class ContinuousLearningPipeline:
    """Pipeline principal d'apprentissage continu"""
    
    def __init__(self):
        self.performance_tracker = PerformanceTracker()
        self.decision_engine = RetrainingDecisionEngine()
        self.model_retrainer = ModelRetrainer()
        self.ab_test_manager = ABTestManager()
        self.pipeline_active = False
        self.monitoring_interval = 3600  # 1 heure
    
    async def start_pipeline(self):
        """DÃ©marrer le pipeline d'apprentissage continu"""
        if self.pipeline_active:
            logger.warning("Pipeline dÃ©jÃ  actif")
            return
        
        self.pipeline_active = True
        logger.info("ğŸ”„ Pipeline d'apprentissage continu dÃ©marrÃ©")
        
        try:
            while self.pipeline_active:
                await self._run_monitoring_cycle()
                await asyncio.sleep(self.monitoring_interval)
        except Exception as e:
            logger.error(f"Erreur pipeline: {e}")
            self.pipeline_active = False
    
    def stop_pipeline(self):
        """ArrÃªter le pipeline"""
        self.pipeline_active = False
        logger.info("Pipeline d'apprentissage continu arrÃªtÃ©")
    
    async def _run_monitoring_cycle(self):
        """ExÃ©cuter un cycle de monitoring"""
        logger.info("ğŸ” DÃ©but cycle de monitoring")
        
        # Simuler la collecte de nouvelles mÃ©triques
        await self._collect_new_performance_data()
        
        # Analyser les besoins de rÃ©entraÃ®nement
        await self._analyze_retraining_needs()
        
        # GÃ©rer les tests A/B actifs
        await self._manage_ab_tests()
        
        logger.info("âœ… Cycle de monitoring terminÃ©")
    
    async def _collect_new_performance_data(self):
        """Collecter les nouvelles donnÃ©es de performance"""
        # Simuler la collecte de donnÃ©es pour diffÃ©rents modÃ¨les
        model_types = ['match_result', 'total_goals', 'both_teams_scored']
        leagues = ['premier_league', 'la_liga', 'bundesliga']
        
        for model_type in model_types:
            for league in leagues:
                model_id = f"{league}_{model_type}_xgb"
                
                # Simuler des mÃ©triques
                metrics = PerformanceMetrics(
                    model_id=model_id,
                    prediction_type=model_type,
                    league=league,
                    accuracy=np.random.uniform(0.6, 0.85),
                    precision=np.random.uniform(0.55, 0.8),
                    recall=np.random.uniform(0.5, 0.75),
                    f1_score=np.random.uniform(0.52, 0.77),
                    roc_auc=np.random.uniform(0.65, 0.9),
                    log_loss=np.random.uniform(0.3, 0.8),
                    roi=np.random.uniform(-0.1, 0.3),
                    profit_loss=np.random.uniform(-50, 150),
                    confidence_calibration=np.random.uniform(0.7, 0.95),
                    sample_size=np.random.randint(5, 25),
                    period_start=datetime.now() - timedelta(hours=24),
                    period_end=datetime.now()
                )
                
                self.performance_tracker.add_performance_metrics(metrics)
    
    async def _analyze_retraining_needs(self):
        """Analyser les besoins de rÃ©entraÃ®nement"""
        for model_id in list(self.performance_tracker.metrics_history.keys()):
            decision = self.decision_engine.should_retrain(model_id, self.performance_tracker)
            
            if decision['should_retrain']:
                logger.info(f"RÃ©entraÃ®nement recommandÃ© pour {model_id}: {decision['reasons']}")
                
                # DÃ©cider du type de rÃ©entraÃ®nement
                retrain_type = "full" if decision['urgency'] in ['high', 'critical'] else "incremental"
                
                # Lancer le rÃ©entraÃ®nement en arriÃ¨re-plan
                asyncio.create_task(self.model_retrainer.retrain_model(model_id, retrain_type))
    
    async def _manage_ab_tests(self):
        """GÃ©rer les tests A/B actifs"""
        completed_tests = []
        
        for test_id, test_info in self.ab_test_manager.active_tests.items():
            if datetime.now() >= test_info['end_date']:
                # Analyser les rÃ©sultats
                results = self.ab_test_manager.analyze_test_results(test_id)
                
                if results.get('recommendation') == 'deploy':
                    logger.info(f"Test A/B {test_id} terminÃ© - DÃ©ploiement recommandÃ© du {results['winner']}")
                
                completed_tests.append(test_id)
        
        # Nettoyer les tests terminÃ©s
        for test_id in completed_tests:
            self.ab_test_manager.test_history.append(
                self.ab_test_manager.active_tests.pop(test_id)
            )
    
    def get_pipeline_status(self) -> Dict[str, Any]:
        """Obtenir le statut du pipeline"""
        return {
            'active': self.pipeline_active,
            'models_monitored': len(self.performance_tracker.metrics_history),
            'active_ab_tests': len(self.ab_test_manager.active_tests),
            'active_retrains': len(self.model_retrainer.active_retrains),
            'monitoring_interval_hours': self.monitoring_interval / 3600,
            'last_cycle': datetime.now().isoformat()
        }
    
    def trigger_manual_retrain(self, model_id: str, retrain_type: str = "full") -> Dict[str, Any]:
        """DÃ©clencher un rÃ©entraÃ®nement manuel"""
        return asyncio.create_task(
            self.model_retrainer.retrain_model(model_id, retrain_type)
        )

# Fonction principale pour tests
async def test_continuous_learning_pipeline():
    """Tester le pipeline d'apprentissage continu"""
    print("ğŸ”„ Test Pipeline d'Apprentissage Continu")
    
    pipeline = ContinuousLearningPipeline()
    
    # Test du tracking de performance
    print("\nğŸ“Š Test Performance Tracking...")
    test_metrics = PerformanceMetrics(
        model_id="test_model",
        prediction_type="match_result", 
        league="premier_league",
        accuracy=0.75,
        precision=0.73,
        recall=0.71,
        f1_score=0.72,
        roc_auc=0.82,
        log_loss=0.45,
        roi=0.15,
        profit_loss=75.5,
        confidence_calibration=0.85,
        sample_size=20,
        period_start=datetime.now() - timedelta(hours=24),
        period_end=datetime.now()
    )
    
    pipeline.performance_tracker.add_performance_metrics(test_metrics)
    print("âœ… MÃ©triques ajoutÃ©es")
    
    # Test dÃ©cision de rÃ©entraÃ®nement
    print("\nğŸ¯ Test DÃ©cision RÃ©entraÃ®nement...")
    decision = pipeline.decision_engine.should_retrain("test_model", pipeline.performance_tracker)
    print(f"DÃ©cision: {decision}")
    
    # Test A/B Testing
    print("\nğŸ§ª Test A/B Testing...")
    test_id = pipeline.ab_test_manager.create_ab_test(
        "new_feature_test", "model_v1", "model_v2"
    )
    print(f"Test A/B crÃ©Ã©: {test_id}")
    
    # Simuler quelques rÃ©sultats
    for i in range(15):
        user_context = {'user_id': f'user_{i}'}
        assigned_model = pipeline.ab_test_manager.assign_model(test_id, user_context)
        
        # Simuler un rÃ©sultat
        result = {
            'accuracy': np.random.uniform(0.6, 0.8),
            'roi': np.random.uniform(0.05, 0.25)
        }
        
        pipeline.ab_test_manager.record_result(test_id, assigned_model, result)
    
    # Analyser les rÃ©sultats
    analysis = pipeline.ab_test_manager.analyze_test_results(test_id)
    print(f"Analyse A/B: {analysis}")
    
    print("\nğŸ‰ Tests terminÃ©s avec succÃ¨s!")

if __name__ == "__main__":
    # ExÃ©cuter les tests
    asyncio.run(test_continuous_learning_pipeline())