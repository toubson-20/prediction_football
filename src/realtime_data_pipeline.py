#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
REALTIME DATA PIPELINE - PHASE 1 TRANSFORMATION
Pipeline intelligent pour donn√©es pr√©-match temps r√©el
Int√©gration AdvancedDataCollector + RevolutionaryFeatureEngineering
"""

import asyncio
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Callable
import pandas as pd
import numpy as np
import logging
from dataclasses import dataclass, asdict
from pathlib import Path
import json
import time

from src.advanced_data_collector import AdvancedFootballDataCollector
from src.revolutionary_feature_engineering import RevolutionaryFeatureEngineer
from config import config

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class PreMatchData:
    """Structure pour donn√©es pr√©-match"""
    fixture_id: int
    collection_time: datetime
    minutes_before_kickoff: int
    
    # Donn√©es de base
    home_team_id: int
    away_team_id: int
    league_id: int
    season: int
    kickoff_time: datetime
    
    # Donn√©es enrichies
    lineups_confirmed: bool = False
    weather_data: Dict = None
    betting_odds: Dict = None
    injury_updates: Dict = None
    team_news: Dict = None
    
    # Features g√©n√©r√©es
    features_generated: bool = False
    feature_vector: Dict = None
    data_quality_score: float = 0.0

@dataclass 
class PipelineStats:
    """Statistiques du pipeline"""
    total_fixtures_processed: int = 0
    successful_collections: int = 0
    failed_collections: int = 0
    avg_processing_time_ms: float = 0.0
    features_generated_count: int = 0
    last_update: datetime = None

class RealtimeDataPipeline:
    """
    Pipeline de donn√©es temps r√©el r√©volutionnaire
    - Collecte automatique selon timeline pr√©-match
    - Feature engineering intelligent temps r√©el
    - Validation qualit√© continue
    - Callbacks pour d√©clenchement pr√©dictions
    """
    
    def __init__(self):
        # Composants principaux
        self.data_collector = AdvancedFootballDataCollector()
        self.feature_engineer = RevolutionaryFeatureEngineer()
        
        # Configuration pipeline
        self.collection_timelines = {
            90: "early_data",      # 90+ min : donn√©es pr√©liminaires
            60: "confirmed_lineups", # 60+ min : compositions confirm√©es
            30: "betting_odds",    # 30+ min : cotes finales
            15: "final_updates",   # 15+ min : derni√®res mises √† jour
            0: "kickoff_ready"     # 0 min : pr√™t pour pr√©diction
        }
        
        # Storage et cache
        self.active_fixtures: Dict[int, PreMatchData] = {}
        self.processed_data: Dict[int, Dict] = {}
        self.callbacks: List[Callable] = []
        
        # Statistiques
        self.stats = PipelineStats()
        
        # Configuration temps r√©el
        self.monitoring_interval = 300  # 5 minutes
        self.is_monitoring = False
        
        logger.info("‚ö° RealtimeDataPipeline initialis√©")

    def register_callback(self, callback: Callable[[int, Dict], None]) -> None:
        """
        Enregistrer un callback pour notifications temps r√©el
        Le callback sera appel√© avec (fixture_id, processed_data)
        """
        self.callbacks.append(callback)
        logger.info(f"üìû Callback enregistr√© - Total: {len(self.callbacks)}")

    async def start_monitoring(self, fixtures_to_monitor: List[Dict]) -> None:
        """
        D√©marrer le monitoring temps r√©el pour les fixtures
        R√âVOLUTIONNAIRE: Surveillance automatique multi-fixtures
        """
        logger.info(f"üéØ D√©marrage monitoring pour {len(fixtures_to_monitor)} fixtures")
        
        # Initialiser les fixtures √† surveiller
        for fixture_info in fixtures_to_monitor:
            fixture_id = fixture_info['fixture_id']
            kickoff_time = pd.to_datetime(fixture_info['kickoff_time'])
            
            pre_match_data = PreMatchData(
                fixture_id=fixture_id,
                collection_time=datetime.now(),
                minutes_before_kickoff=self._calculate_minutes_before(kickoff_time),
                home_team_id=fixture_info['home_team_id'],
                away_team_id=fixture_info['away_team_id'],
                league_id=fixture_info['league_id'],
                season=fixture_info['season'],
                kickoff_time=kickoff_time
            )
            
            self.active_fixtures[fixture_id] = pre_match_data
        
        # Lancer la surveillance en boucle
        self.is_monitoring = True
        await self._monitoring_loop()

    async def _monitoring_loop(self) -> None:
        """Boucle principale de monitoring"""
        logger.info("üîÑ Boucle monitoring d√©marr√©e")
        
        while self.is_monitoring:
            current_time = datetime.now()
            fixtures_to_remove = []
            
            for fixture_id, pre_match_data in self.active_fixtures.items():
                try:
                    # Calculer temps restant
                    minutes_before = self._calculate_minutes_before(pre_match_data.kickoff_time)
                    
                    # Si match d√©j√† commenc√©, arr√™ter surveillance
                    if minutes_before < 0:
                        fixtures_to_remove.append(fixture_id)
                        continue
                    
                    # V√©rifier si besoin de collecter selon timeline
                    should_collect = self._should_collect_now(minutes_before, pre_match_data)
                    
                    if should_collect:
                        logger.info(f"üìä Collection donn√©es fixture {fixture_id} ({minutes_before}min avant)")
                        await self._process_fixture(fixture_id, minutes_before)
                
                except Exception as e:
                    logger.error(f"‚ùå Erreur monitoring fixture {fixture_id}: {e}")
            
            # Nettoyer fixtures termin√©s
            for fixture_id in fixtures_to_remove:
                del self.active_fixtures[fixture_id]
                logger.info(f"‚úÖ Monitoring termin√© pour fixture {fixture_id}")
            
            # Si plus de fixtures, arr√™ter monitoring
            if not self.active_fixtures:
                self.is_monitoring = False
                logger.info("üèÅ Monitoring termin√© - Plus de fixtures actives")
                break
            
            # Attendre avant prochaine v√©rification
            await asyncio.sleep(self.monitoring_interval)

    def _calculate_minutes_before(self, kickoff_time: datetime) -> int:
        """Calculer minutes avant coup d'envoi"""
        now = datetime.now()
        delta = kickoff_time - now
        return int(delta.total_seconds() / 60)

    def _should_collect_now(self, minutes_before: int, pre_match_data: PreMatchData) -> bool:
        """
        D√©terminer si on doit collecter maintenant selon timeline
        INTELLIGENT: √âvite collectes redondantes
        """
        # Identifier la timeline actuelle
        current_timeline = None
        for threshold in sorted(self.collection_timelines.keys(), reverse=True):
            if minutes_before >= threshold:
                current_timeline = threshold
                break
        
        if current_timeline is None:
            return False
        
        # V√©rifier si on a d√©j√† collect√© pour cette timeline
        last_collection_minutes = getattr(pre_match_data, 'last_collection_minutes', 999)
        
        # Collecter si on entre dans une nouvelle timeline
        return minutes_before <= current_timeline and last_collection_minutes > current_timeline

    async def _process_fixture(self, fixture_id: int, minutes_before: int) -> None:
        """
        Traitement complet d'un fixture
        PIPELINE: Collecte ‚Üí Features ‚Üí Validation ‚Üí Callbacks
        """
        start_time = time.time()
        pre_match_data = self.active_fixtures[fixture_id]
        
        try:
            # 1. COLLECTE DONN√âES TEMPS R√âEL
            logger.debug(f"üì• Collecte donn√©es fixture {fixture_id}")
            raw_data = self.data_collector.get_pre_match_data(fixture_id, minutes_before)
            
            if not raw_data:
                logger.warning(f"‚ö†Ô∏è Aucune donn√©e pour fixture {fixture_id}")
                return
            
            # 2. ENRICHISSEMENT AVEC DONN√âES HISTORIQUES
            logger.debug(f"üîç Enrichissement donn√©es historiques")
            enriched_data = await self._enrich_with_historical_data(raw_data, pre_match_data)
            
            # 3. G√âN√âRATION FEATURES R√âVOLUTIONNAIRES  
            logger.debug(f"üß† G√©n√©ration features")
            features_df = self._generate_realtime_features(enriched_data, pre_match_data)
            
            # 4. VALIDATION QUALIT√â
            logger.debug(f"‚úÖ Validation qualit√©")
            quality_score = self._validate_data_quality(features_df)
            
            # 5. MISE √Ä JOUR STRUCTURE
            pre_match_data.collection_time = datetime.now()
            pre_match_data.minutes_before_kickoff = minutes_before
            pre_match_data.features_generated = True
            pre_match_data.feature_vector = features_df.to_dict('records')[0] if not features_df.empty else {}
            pre_match_data.data_quality_score = quality_score
            pre_match_data.last_collection_minutes = minutes_before  # Track derni√®re collecte
            
            # 6. SAUVEGARDE
            self.processed_data[fixture_id] = {
                'fixture_id': fixture_id,
                'processing_time': datetime.now().isoformat(),
                'minutes_before': minutes_before,
                'raw_data': raw_data,
                'features': pre_match_data.feature_vector,
                'quality_score': quality_score
            }
            
            # 7. CALLBACKS NOTIFICATIONS
            await self._notify_callbacks(fixture_id, self.processed_data[fixture_id])
            
            # 8. STATISTIQUES
            processing_time_ms = (time.time() - start_time) * 1000
            self._update_stats(processing_time_ms, success=True)
            
            logger.info(f"‚úÖ Fixture {fixture_id} trait√© en {processing_time_ms:.1f}ms (qualit√©: {quality_score:.2f})")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur traitement fixture {fixture_id}: {e}")
            self._update_stats(0, success=False)

    async def _enrich_with_historical_data(self, raw_data: Dict, pre_match_data: PreMatchData) -> pd.DataFrame:
        """
        Enrichir avec donn√©es historiques n√©cessaires aux features
        INNOVATION: Collecte intelligente donn√©es manquantes
        """
        # Cr√©er DataFrame de base
        match_row = {
            'fixture_id': pre_match_data.fixture_id,
            'home_team_id': pre_match_data.home_team_id,
            'away_team_id': pre_match_data.away_team_id,
            'league_id': pre_match_data.league_id,
            'season': pre_match_data.season,
            'date': pre_match_data.kickoff_time.isoformat(),
            'minutes_before_kickoff': pre_match_data.minutes_before_kickoff
        }
        
        # Ajouter donn√©es temps r√©el collect√©es
        match_row.update(raw_data)
        
        # Enrichir avec donn√©es historiques si n√©cessaire
        try:
            # Classements actuels
            home_standings = self.data_collector.get_standings(pre_match_data.league_id, pre_match_data.season)
            away_standings = home_standings  # M√™me ligue
            
            if not home_standings.empty:
                home_team_standing = home_standings[home_standings['team_id'] == pre_match_data.home_team_id]
                away_team_standing = home_standings[home_standings['team_id'] == pre_match_data.away_team_id]
                
                # Ajouter stats classement
                for prefix, standing in [('home', home_team_standing), ('away', away_team_standing)]:
                    if not standing.empty:
                        for col in ['position', 'points', 'played', 'won', 'draw', 'lost', 'goalsFor', 'goalsAgainst']:
                            if col in standing.columns:
                                match_row[f'{prefix}_{col}'] = standing[col].iloc[0]
            
            # Statistiques √©quipes compl√®tes  
            home_stats = self.data_collector.get_team_comprehensive_stats(
                pre_match_data.home_team_id, pre_match_data.league_id, pre_match_data.season
            )
            away_stats = self.data_collector.get_team_comprehensive_stats(
                pre_match_data.away_team_id, pre_match_data.league_id, pre_match_data.season
            )
            
            # Ajouter stats √©quipes
            for prefix, stats in [('home', home_stats), ('away', away_stats)]:
                for key, value in stats.items():
                    if not key.endswith('_id'):  # √âviter doublons IDs
                        match_row[f'{prefix}_{key}'] = value
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur enrichissement historique: {e}")
        
        return pd.DataFrame([match_row])

    def _generate_realtime_features(self, enriched_data: pd.DataFrame, pre_match_data: PreMatchData) -> pd.DataFrame:
        """
        G√©n√©rer features avec le RevolutionaryFeatureEngineer
        TEMPS R√âEL: Features optimis√©es pour pr√©dictions live
        """
        try:
            # G√©n√©ration features compl√®tes
            features_df = self.feature_engineer.create_revolutionary_features(
                enriched_data,
                include_player_data=True,
                include_tactical_data=True
            )
            
            # Ajout de features sp√©cifiques temps r√©el
            features_df['realtime_collection'] = True
            features_df['minutes_before_kickoff'] = pre_match_data.minutes_before_kickoff
            features_df['data_freshness_score'] = self._calculate_data_freshness(pre_match_data)
            
            return features_df
            
        except Exception as e:
            logger.error(f"‚ùå Erreur g√©n√©ration features: {e}")
            return enriched_data  # Fallback

    def _calculate_data_freshness(self, pre_match_data: PreMatchData) -> float:
        """
        Calculer score de fra√Æcheur des donn√©es
        INNOVATION: Score bas√© sur recence des donn√©es
        """
        minutes_before = pre_match_data.minutes_before_kickoff
        
        # Score plus √©lev√© = donn√©es plus fra√Æches/fiables
        if minutes_before <= 15:
            return 1.0  # Donn√©es tr√®s fra√Æches
        elif minutes_before <= 30:
            return 0.9
        elif minutes_before <= 60:
            return 0.8
        elif minutes_before <= 90:
            return 0.7
        else:
            return 0.6  # Donn√©es pr√©liminaires

    def _validate_data_quality(self, df: pd.DataFrame) -> float:
        """
        Validation qualit√© pour donn√©es temps r√©el
        SCORE: 0-1 bas√© sur compl√©tude et coh√©rence
        """
        if df.empty:
            return 0.0
        
        # M√©triques qualit√©
        total_cols = len(df.columns)
        missing_values = df.isnull().sum().sum()
        completeness = 1 - (missing_values / (len(df) * total_cols))
        
        # V√©rifications coh√©rence basiques
        coherence_score = 1.0
        
        # Scores n√©gatifs impossibles
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        negative_scores = df[numeric_cols].lt(0).sum().sum()
        if negative_scores > 0:
            coherence_score -= 0.1
        
        # Score final pond√©r√©
        quality_score = (completeness * 0.7) + (coherence_score * 0.3)
        
        return min(max(quality_score, 0.0), 1.0)

    async def _notify_callbacks(self, fixture_id: int, processed_data: Dict) -> None:
        """Notifier tous les callbacks enregistr√©s"""
        for callback in self.callbacks:
            try:
                if asyncio.iscoroutinefunction(callback):
                    await callback(fixture_id, processed_data)
                else:
                    callback(fixture_id, processed_data)
            except Exception as e:
                logger.error(f"‚ùå Erreur callback: {e}")

    def _update_stats(self, processing_time_ms: float, success: bool) -> None:
        """Mettre √† jour statistiques pipeline"""
        self.stats.total_fixtures_processed += 1
        
        if success:
            self.stats.successful_collections += 1
            self.stats.features_generated_count += 1
        else:
            self.stats.failed_collections += 1
        
        # Moyenne mobile du temps de traitement
        if self.stats.avg_processing_time_ms == 0:
            self.stats.avg_processing_time_ms = processing_time_ms
        else:
            # Moyenne pond√©r√©e (nouveau = 30%, ancien = 70%)
            self.stats.avg_processing_time_ms = (
                self.stats.avg_processing_time_ms * 0.7 + processing_time_ms * 0.3
            )
        
        self.stats.last_update = datetime.now()

    def get_fixture_status(self, fixture_id: int) -> Optional[Dict]:
        """Obtenir le statut d'un fixture"""
        if fixture_id not in self.active_fixtures:
            return None
        
        pre_match_data = self.active_fixtures[fixture_id]
        return {
            'fixture_id': fixture_id,
            'minutes_before_kickoff': self._calculate_minutes_before(pre_match_data.kickoff_time),
            'data_quality_score': pre_match_data.data_quality_score,
            'features_generated': pre_match_data.features_generated,
            'last_collection': pre_match_data.collection_time.isoformat() if pre_match_data.collection_time else None
        }

    def get_processed_data(self, fixture_id: int) -> Optional[Dict]:
        """R√©cup√©rer donn√©es trait√©es pour un fixture"""
        return self.processed_data.get(fixture_id)

    def get_pipeline_stats(self) -> Dict:
        """Statistiques du pipeline"""
        return asdict(self.stats)

    async def stop_monitoring(self) -> None:
        """Arr√™ter le monitoring"""
        logger.info("üõë Arr√™t monitoring demand√©")
        self.is_monitoring = False

    def save_processed_data(self, output_dir: Path = None) -> None:
        """Sauvegarder toutes les donn√©es trait√©es"""
        if output_dir is None:
            output_dir = config.PROCESSED_DATA_DIR / "realtime"
        
        output_dir.mkdir(parents=True, exist_ok=True)
        
        for fixture_id, data in self.processed_data.items():
            file_path = output_dir / f"fixture_{fixture_id}_processed.json"
            with open(file_path, 'w') as f:
                json.dump(data, f, indent=2, default=str)
        
        logger.info(f"üíæ {len(self.processed_data)} fixtures sauvegard√©s dans {output_dir}")

# Factory et utilitaires
def create_realtime_pipeline() -> RealtimeDataPipeline:
    """Factory pour cr√©er le pipeline temps r√©el"""
    return RealtimeDataPipeline()

async def demo_pipeline():
    """D√©monstration du pipeline temps r√©el"""
    logger.info("üé¨ D√©monstration Pipeline Temps R√©el")
    
    # Fixtures de test
    test_fixtures = [
        {
            'fixture_id': 12345,
            'home_team_id': 33,  # Manchester United
            'away_team_id': 34,  # Arsenal
            'league_id': 39,     # Premier League
            'season': 2025,
            'kickoff_time': (datetime.now() + timedelta(minutes=75)).isoformat()
        }
    ]
    
    # Callback de test
    def test_callback(fixture_id: int, processed_data: Dict):
        logger.info(f"üìû Callback: Fixture {fixture_id} trait√© (qualit√©: {processed_data.get('quality_score', 0):.2f})")
    
    # Cr√©er et d√©marrer pipeline
    pipeline = create_realtime_pipeline()
    pipeline.register_callback(test_callback)
    
    # Monitoring (arr√™t automatique apr√®s d√©mo)
    await pipeline.start_monitoring(test_fixtures)
    
    logger.info("‚úÖ D√©monstration termin√©e")

if __name__ == "__main__":
    # Test du pipeline
    asyncio.run(demo_pipeline())