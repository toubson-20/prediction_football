"""
ü§ñ AI ENHANCEMENT LAYER - INTEGRATION GPT
Couche d'intelligence artificielle pour enrichir le syst√®me ML de pr√©dictions football
"""

import openai
import json
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
import asyncio
import numpy as np
from pathlib import Path

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class AIEnhancedFeatures:
    """Features enrichies par l'IA"""
    original_features: Dict[str, float]
    ai_features: Dict[str, float]
    ai_insights: Dict[str, str]
    confidence_boost: float
    processing_time: float

@dataclass
class AIPredictionExplanation:
    """Explication IA d'une pr√©diction ML"""
    prediction_summary: str
    key_factors: List[str]
    risk_analysis: str
    market_context: str
    recommendation: str
    confidence_reasoning: str

class GPTPromptTemplates:
    """Templates de prompts optimis√©s pour chaque t√¢che IA"""
    
    @staticmethod
    def feature_enhancement_prompt(match_data: Dict, team_stats: Dict) -> str:
        return f"""Tu es un expert en analyse football avec 20 ans d'exp√©rience. Analyse ces donn√©es et g√©n√®re des insights avanc√©s.

DONN√âES MATCH:
{json.dumps(match_data, indent=2)}

STATISTIQUES √âQUIPES:
{json.dumps(team_stats, indent=2)}

MISSION: G√©n√®re exactement 10 features avanc√©es (valeurs 0-1) que des mod√®les ML classiques pourraient manquer.

CONSIGNES STRICTES:
1. Chaque feature doit avoir une valeur num√©rique entre 0 et 1
2. Base-toi sur des patterns footballistiques r√©els et subtils
3. Consid√®re: momentum psychologique, compatibilit√© tactique, conditions contextuelles
4. √âvite les features √©videntes d√©j√† calcul√©es par les stats classiques
5. Sois pr√©cis et justifie chaque valeur

FORMAT DE R√âPONSE OBLIGATOIRE (JSON):
{{
    "ai_momentum_psychological": 0.73,
    "ai_tactical_compatibility": 0.82,
    "ai_weather_adaptation": 0.91,
    "ai_motivation_contextual": 0.67,
    "ai_fatigue_management": 0.55,
    "ai_crowd_impact": 0.88,
    "ai_referee_influence": 0.72,
    "ai_injury_cascade": 0.43,
    "ai_formation_mismatch": 0.79,
    "ai_confidence_momentum": 0.84
}}

Chaque feature doit √™tre accompagn√©e d'une justification courte mais pr√©cise."""

    @staticmethod
    def prediction_explanation_prompt(ml_prediction: Dict, match_context: Dict) -> str:
        return f"""Tu es un commentateur football expert qui explique des pr√©dictions ML √† des parieurs intelligents.

PR√âDICTION ML:
- Type: {ml_prediction.get('prediction_type', 'N/A')}
- Valeur: {ml_prediction.get('prediction_value', 'N/A')}
- Confiance: {ml_prediction.get('confidence', 0)}%
- Cotes: {ml_prediction.get('odds', 'N/A')}
- Mod√®le utilis√©: {ml_prediction.get('model_used', 'N/A')}

CONTEXTE MATCH:
{json.dumps(match_context, indent=2)}

MISSION: Explique cette pr√©diction ML de mani√®re claire et actionnable.

CONSIGNES STRICTES:
1. Explique POURQUOI nos mod√®les ML ont cette confiance
2. Identifie 3-4 facteurs cl√©s qui influencent la pr√©diction
3. Analyse les risques potentiels qui pourraient faire √©chouer la pr√©diction
4. Compare avec le march√© (si les cotes semblent sur/sous-√©valu√©es)
5. Donne une recommandation finale claire (Parier/√âviter/Prudence)
6. Utilise un langage professionnel mais accessible
7. Maximum 300 mots, structure claire

FORMAT DE R√âPONSE:
## üéØ PR√âDICTION: [R√©sum√© en 1 ligne]

## üìä FACTEURS CL√âS:
- [Facteur 1 avec explication courte]
- [Facteur 2 avec explication courte]
- [Facteur 3 avec explication courte]

## ‚ö†Ô∏è RISQUES √Ä SURVEILLER:
[2-3 √©l√©ments qui pourraient faire √©chouer la pr√©diction]

## üí∞ ANALYSE MARCH√â:
[Comparaison avec les cotes propos√©es]

## üé≤ RECOMMANDATION:
[PARIER/√âVITER/PRUDENCE] - [Justification en 1-2 phrases]"""

    @staticmethod
    def value_betting_prompt(our_predictions: List[Dict], market_odds: List[Dict]) -> str:
        return f"""Tu es un math√©maticien expert en probabilit√©s et value betting.

NOS PR√âDICTIONS ML:
{json.dumps(our_predictions, indent=2)}

COTES DU MARCH√â:
{json.dumps(market_odds, indent=2)}

MISSION: Identifie les opportunit√©s de value betting en comparant nos pr√©dictions avec le march√©.

CONSIGNES MATH√âMATIQUES:
1. Calcule la valeur attendue pour chaque pari: (Proba_ML * Cote) - 1
2. Identifie les paris avec valeur attendue > 5%
3. Analyse si nos mod√®les sont historiquement fiables sur ce type de pr√©diction
4. Consid√®re la variance et le risque de chaque opportunit√©
5. Classe par ordre de priorit√© (meilleure valeur + faible risque)

FORMAT DE R√âPONSE (JSON):
{{
    "value_opportunities": [
        {{
            "prediction_type": "match_result",
            "our_probability": 0.78,
            "market_probability": 0.65,
            "expected_value": 0.12,
            "risk_level": "medium",
            "recommendation": "Strong bet",
            "reasoning": "Nos mod√®les sur√©valuent cette √©quipe de 13 points. Historique solide sur ce type de match."
        }}
    ],
    "market_efficiency": 0.73,
    "best_opportunity": "match_result",
    "total_value_found": 0.08
}}"""

    @staticmethod
    def system_optimization_prompt(performance_data: Dict, error_patterns: List[str]) -> str:
        return f"""Tu es un data scientist senior expert en optimisation de syst√®mes ML de pr√©diction sportive.

DONN√âES PERFORMANCE:
{json.dumps(performance_data, indent=2)}

PATTERNS D'ERREURS IDENTIFI√âS:
{json.dumps(error_patterns, indent=2)}

MISSION: Analyse ces donn√©es et sugg√®re des optimisations concr√®tes pour am√©liorer le syst√®me.

CONSIGNES TECHNIQUES:
1. Identifie les faiblesses dans nos pr√©dictions (types de matchs, ligues, situations)
2. Sugg√®re des am√©liorations d'architecture ML sp√©cifiques
3. Propose des nouvelles features qui pourraient corriger les erreurs
4. Recommande des ajustements de seuils ou param√®tres
5. Prioritise les optimisations par impact potentiel vs effort

FORMAT DE R√âPONSE (JSON):
{{
    "critical_issues": ["Issue 1", "Issue 2"],
    "optimization_suggestions": [
        {{
            "category": "feature_engineering",
            "suggestion": "Ajouter features de pression temporelle",
            "impact": "high",
            "effort": "medium",
            "implementation": "Calculer stress index bas√© sur calendrier"
        }}
    ],
    "architecture_improvements": ["Am√©lioration 1", "Am√©lioration 2"],
    "parameter_adjustments": {{
        "confidence_threshold": 0.75,
        "ensemble_weights": [0.4, 0.35, 0.25]
    }},
    "priority_ranking": ["optimization_1", "optimization_2"]
}}"""

class AIFeatureEnhancer:
    """Enrichisseur de features utilisant GPT"""
    
    def __init__(self, api_key: str, model: str = "gpt-4o-mini"):
        """
        Initialiser l'enhancer IA
        
        Args:
            api_key: Cl√© API OpenAI
            model: Mod√®le GPT √† utiliser (gpt-4o-mini recommand√© pour co√ªt/performance)
        """
        self.client = openai.OpenAI(api_key=api_key)
        self.model = model
        self.cache = {}  # Cache pour √©viter les appels r√©p√©titifs
    
    async def enhance_features(self, match_data: Dict, team_stats: Dict) -> AIEnhancedFeatures:
        """
        Enrichir les features avec l'intelligence GPT
        
        Args:
            match_data: Donn√©es du match
            team_stats: Statistiques des √©quipes
            
        Returns:
            Features enrichies avec insights IA
        """
        start_time = datetime.now()
        
        try:
            # V√©rifier le cache
            cache_key = f"{match_data.get('match_id', 'unknown')}_{hash(str(team_stats))}"
            if cache_key in self.cache:
                logger.info("Features IA r√©cup√©r√©es depuis le cache")
                return self.cache[cache_key]
            
            # G√©n√©rer le prompt
            prompt = GPTPromptTemplates.feature_enhancement_prompt(match_data, team_stats)
            
            # Appel GPT
            response = await self._make_gpt_call(prompt)
            
            # Parser la r√©ponse JSON
            try:
                ai_features = json.loads(response)
                # Valider que toutes les valeurs sont entre 0 et 1
                ai_features = {k: max(0, min(1, v)) for k, v in ai_features.items()}
            except json.JSONDecodeError:
                logger.error("Erreur parsing JSON GPT, utilisation features par d√©faut")
                ai_features = self._get_default_ai_features()
            
            # Calculer boost de confiance bas√© sur la coh√©rence des features IA
            confidence_boost = self._calculate_confidence_boost(ai_features)
            
            processing_time = (datetime.now() - start_time).total_seconds()
            
            result = AIEnhancedFeatures(
                original_features=team_stats,
                ai_features=ai_features,
                ai_insights={"generation_method": "gpt", "model": self.model},
                confidence_boost=confidence_boost,
                processing_time=processing_time
            )
            
            # Mise en cache
            self.cache[cache_key] = result
            
            logger.info(f"Features IA g√©n√©r√©es en {processing_time:.2f}s, boost: +{confidence_boost:.2f}%")
            return result
            
        except Exception as e:
            logger.error(f"Erreur g√©n√©ration features IA: {e}")
            # Fallback avec features par d√©faut
            return self._get_fallback_features(team_stats, start_time)
    
    def _calculate_confidence_boost(self, ai_features: Dict[str, float]) -> float:
        """Calculer le boost de confiance bas√© sur les features IA"""
        # Plus les features IA sont coh√©rentes et diversifi√©es, plus le boost
        feature_values = list(ai_features.values())
        
        # Diversit√© des features (√©viter toutes les valeurs similaires)
        diversity = np.std(feature_values)
        
        # Intensit√© moyenne (features proches de 0.5 = incertaines)
        intensity = np.mean([abs(v - 0.5) * 2 for v in feature_values])
        
        # Boost entre 0 et 10%
        boost = min(10, (diversity * 10) + (intensity * 5))
        return round(boost, 2)
    
    def _get_default_ai_features(self) -> Dict[str, float]:
        """Features IA par d√©faut en cas d'erreur"""
        return {
            "ai_momentum_psychological": 0.5,
            "ai_tactical_compatibility": 0.5,
            "ai_weather_adaptation": 0.5,
            "ai_motivation_contextual": 0.5,
            "ai_fatigue_management": 0.5,
            "ai_crowd_impact": 0.5,
            "ai_referee_influence": 0.5,
            "ai_injury_cascade": 0.5,
            "ai_formation_mismatch": 0.5,
            "ai_confidence_momentum": 0.5
        }
    
    def _get_fallback_features(self, team_stats: Dict, start_time: datetime) -> AIEnhancedFeatures:
        """Features de fallback en cas d'√©chec total"""
        return AIEnhancedFeatures(
            original_features=team_stats,
            ai_features=self._get_default_ai_features(),
            ai_insights={"error": "fallback_mode", "model": "none"},
            confidence_boost=0.0,
            processing_time=(datetime.now() - start_time).total_seconds()
        )
    
    async def _make_gpt_call(self, prompt: str) -> str:
        """Effectuer un appel GPT avec gestion d'erreurs"""
        try:
            response = await asyncio.to_thread(
                self.client.chat.completions.create,
                model=self.model,
                messages=[
                    {"role": "system", "content": "Tu es un expert en analyse football et machine learning. R√©ponds toujours en JSON valide."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,  # Peu cr√©atif, plus factuel
                max_tokens=800,
                timeout=30
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logger.error(f"Erreur appel GPT: {e}")
            raise

class AIPredictionExplainer:
    """Explicateur de pr√©dictions utilisant GPT"""
    
    def __init__(self, api_key: str, model: str = "gpt-4o-mini"):
        self.client = openai.OpenAI(api_key=api_key)
        self.model = model
    
    async def explain_prediction(self, ml_prediction: Dict, match_context: Dict) -> AIPredictionExplanation:
        """
        Expliquer une pr√©diction ML avec l'intelligence GPT
        
        Args:
            ml_prediction: R√©sultat de nos mod√®les ML
            match_context: Contexte du match
            
        Returns:
            Explication d√©taill√©e et actionnable
        """
        try:
            prompt = GPTPromptTemplates.prediction_explanation_prompt(ml_prediction, match_context)
            
            response = await self._make_gpt_call(prompt)
            
            # Parser la r√©ponse structur√©e
            explanation = self._parse_explanation(response)
            
            logger.info(f"Explication g√©n√©r√©e pour pr√©diction {ml_prediction.get('prediction_type', 'unknown')}")
            return explanation
            
        except Exception as e:
            logger.error(f"Erreur g√©n√©ration explication: {e}")
            return self._get_fallback_explanation(ml_prediction)
    
    def _parse_explanation(self, gpt_response: str) -> AIPredictionExplanation:
        """Parser la r√©ponse GPT en structure organis√©e"""
        # Extraction des sections avec regex ou parsing simple
        sections = gpt_response.split("##")
        
        prediction_summary = ""
        key_factors = []
        risk_analysis = ""
        market_context = ""
        recommendation = ""
        
        for section in sections:
            if "PR√âDICTION:" in section:
                prediction_summary = section.split("PR√âDICTION:")[1].strip()[:200]
            elif "FACTEURS CL√âS:" in section:
                factors_text = section.split("FACTEURS CL√âS:")[1].strip()
                key_factors = [f.strip("- ").strip() for f in factors_text.split("\n") if f.strip().startswith("-")][:4]
            elif "RISQUES √Ä SURVEILLER:" in section:
                risk_analysis = section.split("RISQUES √Ä SURVEILLER:")[1].strip()[:200]
            elif "ANALYSE MARCH√â:" in section:
                market_context = section.split("ANALYSE MARCH√â:")[1].strip()[:200]
            elif "RECOMMANDATION:" in section:
                recommendation = section.split("RECOMMANDATION:")[1].strip()[:200]
        
        return AIPredictionExplanation(
            prediction_summary=prediction_summary or "Pr√©diction g√©n√©r√©e par nos mod√®les ML",
            key_factors=key_factors or ["Forme r√©cente", "Statistiques historiques", "Contexte match"],
            risk_analysis=risk_analysis or "Risques standards li√©s aux pr√©dictions sportives",
            market_context=market_context or "Analyse du march√© en cours",
            recommendation=recommendation or "Suivre la pr√©diction avec prudence standard",
            confidence_reasoning="Bas√© sur analyse GPT des facteurs contextuels"
        )
    
    def _get_fallback_explanation(self, ml_prediction: Dict) -> AIPredictionExplanation:
        """Explication de fallback en cas d'erreur"""
        return AIPredictionExplanation(
            prediction_summary=f"Pr√©diction {ml_prediction.get('prediction_value', 'N/A')} avec {ml_prediction.get('confidence', 0)}% de confiance",
            key_factors=["Mod√®les ML sp√©cialis√©s", "Donn√©es historiques", "Statistiques actuelles"],
            risk_analysis="Risques standards des pr√©dictions sportives",
            market_context="Analyse de march√© non disponible",
            recommendation="Suivre la pr√©diction selon votre strat√©gie habituelle",
            confidence_reasoning="Bas√© uniquement sur les mod√®les ML (IA indisponible)"
        )
    
    async def _make_gpt_call(self, prompt: str) -> str:
        """Effectuer un appel GPT pour les explications"""
        response = await asyncio.to_thread(
            self.client.chat.completions.create,
            model=self.model,
            messages=[
                {"role": "system", "content": "Tu es un expert commentateur football qui explique des pr√©dictions ML. Sois pr√©cis, structur√© et actionnable."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.4,  # L√©g√®rement plus cr√©atif pour les explications
            max_tokens=600,
            timeout=30
        )
        
        return response.choices[0].message.content.strip()

class AIValueBettingDetector:
    """D√©tecteur d'opportunit√©s value betting utilisant GPT"""
    
    def __init__(self, api_key: str, model: str = "gpt-4o-mini"):
        self.client = openai.OpenAI(api_key=api_key)
        self.model = model
    
    async def detect_value_opportunities(self, our_predictions: List[Dict], market_odds: List[Dict]) -> Dict[str, Any]:
        """
        D√©tecter les opportunit√©s de value betting
        
        Args:
            our_predictions: Nos pr√©dictions ML
            market_odds: Cotes du march√©
            
        Returns:
            Analyse des opportunit√©s de valeur
        """
        try:
            prompt = GPTPromptTemplates.value_betting_prompt(our_predictions, market_odds)
            
            response = await self._make_gpt_call(prompt)
            
            # Parser la r√©ponse JSON
            try:
                value_analysis = json.loads(response)
                logger.info(f"Analyse value betting: {len(value_analysis.get('value_opportunities', []))} opportunit√©s trouv√©es")
                return value_analysis
            except json.JSONDecodeError:
                logger.error("Erreur parsing JSON pour value betting")
                return self._get_fallback_value_analysis()
                
        except Exception as e:
            logger.error(f"Erreur d√©tection value betting: {e}")
            return self._get_fallback_value_analysis()
    
    def _get_fallback_value_analysis(self) -> Dict[str, Any]:
        """Analyse de fallback"""
        return {
            "value_opportunities": [],
            "market_efficiency": 0.7,
            "best_opportunity": None,
            "total_value_found": 0.0,
            "error": "AI analysis unavailable"
        }
    
    async def _make_gpt_call(self, prompt: str) -> str:
        """Appel GPT pour analyse value betting"""
        response = await asyncio.to_thread(
            self.client.chat.completions.create,
            model=self.model,
            messages=[
                {"role": "system", "content": "Tu es un math√©maticien expert en probabilit√©s et value betting. R√©ponds en JSON valide uniquement."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1,  # Tr√®s peu cr√©atif, maximum pr√©cision math√©matique
            max_tokens=700,
            timeout=30
        )
        
        return response.choices[0].message.content.strip()

class AISystemOptimizer:
    """Optimiseur de syst√®me utilisant GPT"""
    
    def __init__(self, api_key: str, model: str = "gpt-4o-mini"):
        self.client = openai.OpenAI(api_key=api_key)
        self.model = model
    
    async def suggest_optimizations(self, performance_data: Dict, error_patterns: List[str]) -> Dict[str, Any]:
        """
        Sugg√©rer des optimisations syst√®me bas√©es sur les performances
        
        Args:
            performance_data: Donn√©es de performance du syst√®me
            error_patterns: Patterns d'erreurs identifi√©s
            
        Returns:
            Suggestions d'optimisation prioritis√©es
        """
        try:
            prompt = GPTPromptTemplates.system_optimization_prompt(performance_data, error_patterns)
            
            response = await self._make_gpt_call(prompt)
            
            try:
                optimization_suggestions = json.loads(response)
                logger.info(f"Suggestions d'optimisation g√©n√©r√©es: {len(optimization_suggestions.get('optimization_suggestions', []))} suggestions")
                return optimization_suggestions
            except json.JSONDecodeError:
                logger.error("Erreur parsing suggestions d'optimisation")
                return self._get_fallback_optimizations()
                
        except Exception as e:
            logger.error(f"Erreur g√©n√©ration optimisations: {e}")
            return self._get_fallback_optimizations()
    
    def _get_fallback_optimizations(self) -> Dict[str, Any]:
        """Optimisations de fallback"""
        return {
            "critical_issues": ["AI analysis unavailable"],
            "optimization_suggestions": [],
            "architecture_improvements": [],
            "parameter_adjustments": {},
            "priority_ranking": []
        }
    
    async def _make_gpt_call(self, prompt: str) -> str:
        """Appel GPT pour optimisations"""
        response = await asyncio.to_thread(
            self.client.chat.completions.create,
            model=self.model,
            messages=[
                {"role": "system", "content": "Tu es un data scientist senior expert en optimisation ML. R√©ponds en JSON valide avec des suggestions concr√®tes et impl√©mentables."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=1000,
            timeout=40
        )
        
        return response.choices[0].message.content.strip()

class AIEnhancementLayer:
    """Couche principale d'enhancement IA"""
    
    def __init__(self, openai_api_key: str, model: str = "gpt-4o-mini"):
        """
        Initialiser la couche d'enhancement IA
        
        Args:
            openai_api_key: Cl√© API OpenAI  
            model: Mod√®le GPT √† utiliser (gpt-4o-mini recommand√© pour √©quilibre co√ªt/performance)
        """
        self.feature_enhancer = AIFeatureEnhancer(openai_api_key, model)
        self.prediction_explainer = AIPredictionExplainer(openai_api_key, model)
        self.value_detector = AIValueBettingDetector(openai_api_key, model)
        self.system_optimizer = AISystemOptimizer(openai_api_key, model)
        
        self.usage_stats = {
            "total_calls": 0,
            "feature_enhancements": 0,
            "explanations_generated": 0,
            "value_analyses": 0,
            "optimizations": 0
        }
        
        logger.info(f"ü§ñ AI Enhancement Layer initialis√© avec {model}")
    
    async def enhance_prediction_pipeline(self, match_data: Dict, team_stats: Dict, ml_prediction: Dict, market_odds: List[Dict] = None) -> Dict[str, Any]:
        """
        Pipeline complet d'enhancement IA
        
        Args:
            match_data: Donn√©es du match
            team_stats: Statistiques des √©quipes  
            ml_prediction: Pr√©diction de nos mod√®les ML
            market_odds: Cotes du march√© (optionnel)
            
        Returns:
            R√©sultat enrichi par l'IA
        """
        try:
            logger.info("üîÑ D√©marrage pipeline d'enhancement IA")
            
            # 1. Enhancement des features (pour futurs entra√Ænements)
            enhanced_features = await self.feature_enhancer.enhance_features(match_data, team_stats)
            self.usage_stats["feature_enhancements"] += 1
            
            # 2. Explication de la pr√©diction ML
            explanation = await self.prediction_explainer.explain_prediction(ml_prediction, match_data)
            self.usage_stats["explanations_generated"] += 1
            
            # 3. Analyse value betting (si cotes disponibles)
            value_analysis = None
            if market_odds:
                value_analysis = await self.value_detector.detect_value_opportunities([ml_prediction], market_odds)
                self.usage_stats["value_analyses"] += 1
            
            self.usage_stats["total_calls"] += 1
            
            return {
                "ml_prediction": ml_prediction,  # ‚Üê Pr√©diction originale de nos mod√®les ML
                "ai_enhanced_features": enhanced_features.ai_features,
                "ai_explanation": explanation,
                "value_opportunities": value_analysis,
                "confidence_boost": enhanced_features.confidence_boost,
                "ai_processing_time": enhanced_features.processing_time,
                "enhancement_timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Erreur pipeline AI enhancement: {e}")
            # Retourner la pr√©diction ML sans enhancement en cas d'erreur
            return {
                "ml_prediction": ml_prediction,
                "ai_enhanced_features": {},
                "ai_explanation": None,
                "value_opportunities": None,
                "confidence_boost": 0.0,
                "error": str(e)
            }
    
    def get_usage_statistics(self) -> Dict[str, Any]:
        """Obtenir les statistiques d'utilisation de l'IA"""
        return {
            **self.usage_stats,
            "average_cost_per_call": 0.002,  # Estimation co√ªt GPT-4o-mini
            "estimated_monthly_cost": self.usage_stats["total_calls"] * 0.002 * 30
        }

# Fonction de test
async def test_ai_enhancement_layer():
    """Tester la couche d'enhancement IA"""
    print("ü§ñ Test AI Enhancement Layer")
    
    # Vous devez d√©finir votre cl√© API OpenAI
    API_KEY = "your-openai-api-key"  # ‚ö†Ô∏è Remplacer par votre vraie cl√© API
    
    if API_KEY == "your-openai-api-key":
        print("‚ùå Veuillez d√©finir votre cl√© API OpenAI")
        return
    
    ai_layer = AIEnhancementLayer(API_KEY)
    
    # Donn√©es de test
    match_data = {
        "match_id": 12345,
        "home_team": "Manchester United",
        "away_team": "Liverpool",
        "league": "Premier League",
        "date": "2025-08-24"
    }
    
    team_stats = {
        "home_goals_avg": 2.1,
        "away_goals_avg": 1.8,
        "home_possession": 0.58,
        "away_possession": 0.62
    }
    
    ml_prediction = {
        "prediction_type": "match_result",
        "prediction_value": "1",  # Home win
        "confidence": 76.5,
        "odds": 2.10,
        "model_used": "ensemble_xgb_nn"
    }
    
    # Test du pipeline complet
    result = await ai_layer.enhance_prediction_pipeline(
        match_data, team_stats, ml_prediction
    )
    
    print(f"‚úÖ Enhancement termin√©:")
    print(f"- Features IA g√©n√©r√©es: {len(result['ai_enhanced_features'])}")
    print(f"- Boost confiance: +{result['confidence_boost']}%")
    print(f"- Explication disponible: {'Oui' if result['ai_explanation'] else 'Non'}")
    
    # Statistiques d'usage
    stats = ai_layer.get_usage_statistics()
    print(f"\nüìä Statistiques: {stats['total_calls']} appels IA")

if __name__ == "__main__":
    # Pour tester, d√©commenter et d√©finir votre cl√© API
    # asyncio.run(test_ai_enhancement_layer())
    print("ü§ñ AI Enhancement Layer pr√™t - D√©finir la cl√© API pour utiliser")